{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cee327e-877a-494f-b86f-5c1c29570862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d74eb-d1b3-416d-829f-30763862afd0",
   "metadata": {},
   "source": [
    "## Simplify multiple labels into a single label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21071d9c",
   "metadata": {},
   "source": [
    "Since in the original dataset there were multiple category labels for each movie. We decided to keep the first category of each movie, i.e., the main category, as the categorization label.\n",
    "\n",
    "At the same time, there are categories that are not related to the content of the work, such as International TV Shows|International Movies|Independent Movies, so we removed these categories from the multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ff7c44-9fef-41e8-8d8a-ee1c3515865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('original/netflix_titles.csv')\n",
    "\n",
    "df['listed_in'] = df['listed_in'].str.replace('International TV Shows|International Movies|Independent Movies', '', regex=True)\n",
    "df['listed_in'] = df['listed_in'].apply(lambda x: x[1:] if isinstance(x, str) and x.startswith(',') else x)\n",
    "df['listed_in'] = df['listed_in'].str.split(',').str[0]\n",
    "\n",
    "df.to_csv('original/netflix_titles_single_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b287945-1e8d-460e-8ae9-c8d57e11e9ab",
   "metadata": {},
   "source": [
    "## Divided the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572ee9f-20ce-47dc-9d31-6ff240566596",
   "metadata": {},
   "source": [
    "### The division ratio is 7:1.5:1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e5374ed-9b4b-4ff1-9208-9e6303446d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('original/netflix_titles_single_label.csv')\n",
    "\n",
    "selected_columns = ['show_id', 'listed_in', 'title', 'description','rating']\n",
    "df_selected = df[selected_columns]\n",
    "\n",
    "df_selected.columns = ['id', 'category', 'title', 'description','rating']\n",
    "\n",
    "df_train, df_temp = train_test_split(df_selected, test_size=0.3, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84bf89b2",
   "metadata": {},
   "source": [
    "Ensure that the categories in the training set, validation set and test set are consistent.\n",
    "Keep only the categories that are common to the training set, validation set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1ec2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories_train = set(df_train['category'].unique())\n",
    "unique_categories_val = set(df_val['category'].unique())\n",
    "unique_categories_test = set(df_test['category'].unique())\n",
    "\n",
    "common_categories = unique_categories_train.intersection(unique_categories_val, unique_categories_test)\n",
    "\n",
    "df_train = df_train[df_train['category'].isin(common_categories)]\n",
    "df_val = df_val[df_val['category'].isin(common_categories)]\n",
    "df_test = df_test[df_test['category'].isin(common_categories)]\n",
    "\n",
    "df_train.to_csv('preprocessed/netflix_train.csv', index=False)\n",
    "df_val.to_csv('preprocessed/netflix_val.csv', index=False)\n",
    "df_test.to_csv('preprocessed/netflix_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0382de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "Number of Samples: 6144\n",
      "Number of Categories: 42\n",
      "\n",
      "Validation Set:\n",
      "Number of Samples: 1319\n",
      "Number of Categories: 42\n",
      "\n",
      "Test Set:\n",
      "Number of Samples: 1321\n",
      "Number of Categories: 42\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('preprocessed/netflix_train.csv')\n",
    "df_val = pd.read_csv('preprocessed/netflix_val.csv')\n",
    "df_test = pd.read_csv('preprocessed/netflix_test.csv')\n",
    "\n",
    "num_samples_train = len(df_train)\n",
    "num_categories_train = len(df_train['category'].unique())\n",
    "\n",
    "num_samples_val = len(df_val)\n",
    "num_categories_val = len(df_val['category'].unique())\n",
    "\n",
    "num_samples_test = len(df_test)\n",
    "num_categories_test = len(df_test['category'].unique())\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(f\"Number of Samples: {num_samples_train}\")\n",
    "print(f\"Number of Categories: {num_categories_train}\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"Number of Samples: {num_samples_val}\")\n",
    "print(f\"Number of Categories: {num_categories_val}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"Number of Samples: {num_samples_test}\")\n",
    "print(f\"Number of Categories: {num_categories_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c59f8a-8dea-48c1-9868-5e82b477c187",
   "metadata": {},
   "source": [
    "## Simple NLP Preprocessing for each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ccd1d0-5a9f-4663-bb80-c12023ee4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b55bb0bc-0d2e-416e-ac2f-bb909a2a2bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Croya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Croya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5fc8f0c",
   "metadata": {},
   "source": [
    "Currently the preprocessing the the simple version. \n",
    "! More operations can be implemented !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "847073f8-9e1a-4c54-83ab-0c25e17de9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    words = [ps.stem(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35148494-3bc1-4f71-8944-1a66ee7fe038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['description'] = df_train['description'].apply(preprocess_text)\n",
    "df_val['description'] = df_val['description'].apply(preprocess_text)\n",
    "df_test['description'] = df_test['description'].apply(preprocess_text)\n",
    "\n",
    "df_train['category'] = df_train['category'].str.strip()\n",
    "df_val['category'] = df_val['category'].str.strip()\n",
    "df_test['category'] = df_test['category'].str.strip()\n",
    "\n",
    "df_train.to_csv('preprocessed/netflix_train.csv', index=False)\n",
    "df_val.to_csv('preprocessed/netflix_val.csv', index=False)\n",
    "df_test.to_csv('preprocessed/netflix_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
